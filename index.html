<!DOCTYPE html>
<!--Adapted from: https://visual.cs.brown.edu/workshops/necv2019/-->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Required meta tags -->
    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="./files/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <style>
    body {
        font-family: 'serif';
        width: 1000px;
        margin: auto;
    }

    a:link {
        color: #C00404;
    }

    a:visited {
        color: #E21B23;
    }

    p {
        text-align: justify;
        -webkit-hyphens: auto;
            -moz-hyphens: auto;
        hyphens: auto;
    }

    table {

    }

    .brownbrown {
        color: #4E3629;
        highlight
    }

    .brownred {
        color: #C00404;
    }
    </style>

    <title>NECV 2019</title>
  </head>
  <body>
    
    <div class="container brownbrown w-75" align="center">
        <hr>
        <h1>New England Computer Vision Workshop</h1>
        <h3>MIT, Boston, MA</h3>
        <em><h3>Friday 9th December 2022</h3></em>
    </div>

    <div class="container" align="center">
        <img src="./files/header_img.jpg" width="100%" style="padding: 1em 0em 1em 0em"><br>
    </div>

    <div class="container w-75">
        <hr>
        <p>
        The New England Computer Vision Workshop (NECV) brings together researchers in computer vision and related areas for an informal exchange of ideas through a full day of presentations and posters. NECV typically attracts around 100 people from universities and industry research labs in New England. As in previous years, the workshop will focus on graduate student presentations.
        </p>

        <p>
        Welcome!<br>
        - <a href="http://web.mit.edu/phillipi">Phillip Isola</a> and <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
        </p>
    </div>

    <div class="container w-75">
        <hr>
        <h3>Registration</h3>
        <p>
            Participation is free for all researchers at academic institutions. Academic researchers should register <a href="https://forms.gle/gvXxyLByXhRABGtz5" target="_blank">here</a>.
        </p>
        <p>
        For our industry friends, a limited number of registrations are available for a fee. Please contact <a href="mailto:samson@ai.mit.edu" target="_blank">Samson Timoner - samson@ai.mit.edu</a> for details.
        </p>
        
    </div>

    <div class="container w-75">
        <hr>
        <h3>Submission</h3>
        <p>
        Please submit a one-page PDF abstract using the <a href="https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip">CVPR 2023 rebuttal template</a> by email to <a href="mailto:necv2022mit@gmail.com">necv2022mit@gmail.com</a>. Abstracts are due by 11:59pm on <b>Mon Nov 21st</b>, 2022. Oral decisions will be released by Nov 28th..
        </p>

        <p>
        You may present work that has already been published, or work that is in progress. All submissions will be granted a poster presentation, and selected submissions from each institution will be granted 12-minute oral presentations. Post-docs and faculty may submit for poster presentations, but oral presentations are reserved for graduate students.
        </p>

        <p>
        There will be <em>no publications</em> resulting from the workshop, so presentations will not be considered "prior peer-reviewed work" according to any definition we are aware of. Thus, work presented at NECV can be subsequently submitted to other venues without citation.
        </p>

        <p>
        The workshop is after the CVPR supplemental deadline, so come and show off your new work in a friendly environment.
        </p>
    </div>

    <!--<div class="container w-75">
        <hr>
        <h3>Best Poster Competition Results</h3>

        <ul>
            <li></li>
        </ul>
    </div>-->

    <div class="container w-75">
        <hr>
        <h3>Logistics</h3>
        
        <h4>Schedule</h4>
        Coming soon!
        <br><br>
        <!--<h4>Tentative Schedule</h4>
        <p>
        </p><table class="w-100 table table-bordered table-responsive-sm table-sm">
            <tbody>
            <tr>
                <td>09:00-10:00</td>
                <td>Breakfast, sign-in, poster setup</td>
                <td></td>
            </tr>
            <tr>
                <td>10:00-10:15</td>
                <td>Welcome</td>
                <td></td>
            </tr>
            <tr>
                <td>10:15-11:45</td>
                <td>Oral presentations 1</td>
                <td>
                    <ul>
                        <li>Jahanian et al., On the “Steerability" of Generative Adversarial Networks (MIT)</li>
                        <li>Usman et al., PuppetGAN: Cross-Domain Image Manipulation by Demonstration (BostonU)</li>
                        <li>Gadelha et al., Deep Manifold Prior (UMass Amherst)</li>
                        <li>Lin et al., Learning to See before Learning to Act: Visual Pre-training for Manipulation (MIT)</li>
                        <li>Sharma et al., Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller (MIT)</li>
                        <li>Dyballa and Zucker, Deep Artificial Neural Networks: Little Brains or Big Retinas or Both? (Yale)</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td>11:45-13:00</td>
                <td>Lunch</td>
                <td></td>
            </tr>
            <tr>
                <td>13:00-14:30</td>
                <td>Poster presentations</td>
                <td>
                    <ul>
                        <li>Khan et al., View-consistent 4D Light Field Superpixel Segmentation (Brown)</li>
                        <li>Jalal et al., Kenyan Food Type Recognition in Instagram Photos (BostonU)</li>
                        <li>Yang et al., AG-NMS: Improving Object Detection in Crowd Scenes (BostonU)</li>
                        <li>Fabbri et al., Trifocal Relative Pose from Lines at Points and its Efficient Solution (Brown)</li>
                        <li>Zou et al., Automated Assessment of Human Embryo Development for In-Vitro Fertilization Treatment (Brown)</li>
                        <li>Stone and Tompkin, Interpreting DeepVoxel Representations in Neural Networks (Brown)</li>
                        <li>Tang et al., Qualitative Camera Relative Pose Estimation (Brown)</li>
                        <li>Correa et al., GridMapper: Mapping Electricity Grid Infrastructure in Developing Countries (UMass Amherst)</li>
                        <li>Pérez et al., Star Cluster Classification from LEGUS Photometric Catalogs (UMass Amherst)</li>
                        <li>Ammar and Taubin, Real-time Computer Assisted Carving (Brown)</li>
                        <li>Cohen et al., Shape From Tracing (Brown)</li>
                        <li>Zhang et al., Practical Physically-based Lighting and Skin Reflectance Reconstructor for Face Images (Brown)</li>
                        <li>Kotani et al., Reinvention of Printing and Handwriting (Brown)</li>
                        <li>Tursman et al., Social Verification of Geometric Consistency to Combat Deepfakes (Brown)</li>
                        <li>Jiang et al., SENSE: a Shared Encoder Network for Scene-flow Estimation (UMass Amherst)</li>
                        <li>Bashkirova et al., Adversarial Self-Defense for Cycle-Consistent GANs (BostonU)</li>
                        <li>Feng et al., Robust Visual Object Tracking with Natural Language Region Proposal Network (BostonU)</li>
                        <li>Hsieh et al., Image-based Virtual Pathology based on Intra-procedure High Resolution CT for Microwave Ablation of Lung Tumors (Rhode Island Hospital)</li>
                        <li>Teterwak et al., Boundless: Generative Adversarial Networks for Image Extension</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td>14:30-15:45</td>
                <td>Oral presentations 2</td>
                <td>
                    <ul>
                        <li>Huh et al., Projecting Images to Class-Conditional Generative Networks (MIT)</li>
                        <li>Guo et al., Compact single-shot metalens depth sensors inspired by eyes of jumping spiders (Harvard)</li>
                        <li>Li et al., KW-DYAN: A Recurrent Network for Adaptive Sequence Prediction (Northeastern)</li>
                        <li>Robinson et al., Laplace Landmark Localization (Northeastern)</li>
                        <li>Ruiz et al., Leveraging Affect Transfer Learning for Behavior Prediction in an Intelligent Tutoring System (BostonU/Clark/WPI)</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td>15:45-16:00</td>
                <td>Coffee</td>
                <td></td>
            </tr>
            <tr>
                <td>16:00-17:15</td>
                <td>Oral presentations 3</td>
                <td>
                    <ul>
                        <li>Su et al., When Does Self-supervision Improve Few-shot Learning? (UMass Amherst)</li>
                        <li>Li et al., Weakly Supervised Learning of Human-Object Interactions from Captioned Videos (MIT)</li>
                        <li>Cheng et al., A Bayesian Perspective on the Deep Image Prior (UMass Amherst)</li>
                        <li>Walecki and Taubin, Super-Resolution 3D Laser Scanning based on Interval Arithmetic (Brown)</li>
                        <li>Wu et al., PhraseCut: Instance-Aware Segmentation from Natural Language Phrases (UMass Amherst)</li>
                    </ul>
                </td>
            </tr> 
            </tbody>
        </table>
        <p></p>-->

        <!--<h4>WiFi</h4>
        <p>
        Logon to "Brown Guest" and accept the terms. Alternatively, if your home institution participates in eduroam, then you can also use eduroam to connect wirelessly while on our campus.
        </p>-->

        <h4>Getting here</h4>
        <p>
            The workshop will be held at the MIT-IBM Watson AI Lab (314 Main Street, Cambridge, MA, 0214).
        </p>
        <!--<iframe src="./NECV 2019_files/embed.html" width="75%" frameborder="0" style="border:0;" allowfullscreen=""></iframe>
        
        <br><img src="./NECV 2019_files/alumnaehall.jpg" width="75%" style="padding: 1em 0em 1em 0em"><br>
    
        <p>
            <b>By train:</b> From the station, campus is a 15 minute walk up College Hill, or a short taxi ride. <em>From Boston:</em> MBTA and Amtrak services run regularly. Take a train south from South Station or Back Bay to Providence, RI. MBTA service is on the <a href="https://www.mbta.com/schedules/CR-Providence/timetable?direction_id=0">Providence/Stoughton line</a>. MBTA fares are $12.50 and takes 1:00-1:10 hr from South Station; Amtrak takes 40 minutes and costs $14 for early booking on the Northeastern train or up to $40-60 for the Acela train. <em>From New Haven, New London, NYC:</em> Amtrak service north to Providence, RI.
        </p>

        <p>
            <b>By coach:</b> <a href="https://peterpanbus.com/">Peter Pan</a> coach services run throughout New England; come to Providence (downtown) and either walk (15 mins), take a local bus (RIPTA, $2), or use taxi services.
        </p>

        <p>
            <b>Local buses:</b> <a href="https://www.ripta.com/">Rhode Island Public Transit Authority (RIPTA)</a> has a schedule and map, with real-time information through the "Transit" app. Buses 1, 32, 33 run to campus from downtown; walking is ~15 minutes.
        </p>

        <p>
            <b>By car:</b> Brown has a visitor parking lot which costs $15 per day. <a href="https://www.brown.edu/about/administration/transportation/about/visitor-parking">Instructions here</a>. Entrance is on Brook Street <a href="https://www.google.com/maps/@41.8227839,-71.3992847,19z">(map)</a>. On-street parking within campus is extremely limited. Street parking ~10 minutes walk away has fewer restrictions.
        </p>-->
    </div>

    <div class="container w-75">
        <hr>
        <h3>Sponsorship</h3>
        <p>We are grateful to the MIT-IBM Watson AI Lab for the venue and for their support.</p>
    </div>

    <div class="container w-75">
        <hr>
        <h3>Acknowledgements</h3>
        <p>
            Thank you to Samson Timoner for helping us arrange NECV 2022. Thank you also to the steering committee: James Tompkin, Benjamin Kimia, Todd Zickler, Yun Raymond Fu, Octavia Camps, Kate Saenko, Erik Learned-Miller, and Subhransu Maji.
        </p>
        <h4>Past Years</h4>
        <ul>
            <li>2019 - Brown University <a href="https://visual.cs.brown.edu/workshops/necv2019/">(website)</a></li>
            <li>2018 - Harvard University <a href="https://projects.iq.harvard.edu/necv2018/">(website)</a></li>
            <li>2017 - Northeastern University <a href="https://web.northeastern.edu/smilelab/necv2017/index.html">(website)</a></li>
            <li>2016 - Boston University <a href="http://vision.cs.uml.edu/necv2016.html">(website)</a></li>
            <li>2015 - UMass Amherst <a href="https://people.cs.umass.edu/~smaji/nevm2015/">(website)</a></li>
        </ul>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <!--<script src="./NECV 2019_files/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="./NECV 2019_files/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="./NECV 2019_files/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>-->
  
</body></html>